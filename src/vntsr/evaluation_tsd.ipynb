{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bdbf427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import random\n",
    "from pathlib import Path\n",
    "import onnxruntime as ort\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "687bb41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import shutil\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36bc12ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import onnxruntime as ort\n",
    "from openvino import Core\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc9d7b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import random\n",
    "from pathlib import Path\n",
    "import ncnn\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f2b6e9",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f15fba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H√†m benchmark PyTorch\n",
    "def evaluate_model(model_path, data_yaml, device=\"cpu\", eval_type=\"Self\", imgsz=640, batch=4, num_threads=8, model_name=None):\n",
    "    print(f\"\\nüìà [{eval_type}] Evaluating (PyTorch): {os.path.basename(model_path)} on {data_yaml}\")\n",
    "\n",
    "    # --- C·ªë ƒë·ªãnh threading ---\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(num_threads)\n",
    "    os.environ[\"MKL_NUM_THREADS\"] = str(num_threads)\n",
    "    torch.set_num_threads(num_threads)\n",
    "\n",
    "    # --- Load model YOLO ---\n",
    "    model = YOLO(model_path)\n",
    "    model.to(device)\n",
    "\n",
    "    # --- Validation phase ---\n",
    "    t0 = time.time()\n",
    "    metrics = model.val(data=data_yaml, imgsz=imgsz, batch=batch, device=device, verbose=False)\n",
    "    eval_time = time.time() - t0\n",
    "\n",
    "    # --- Extract YOLO metrics ---\n",
    "    map50 = round(metrics.box.map50, 4)\n",
    "    map95 = round(metrics.box.map, 4)\n",
    "    prec = round(metrics.box.mp, 4)\n",
    "    recall = round(metrics.box.mr, 4)\n",
    "    f1 = round(2 * (prec * recall) / (prec + recall + 1e-9), 4)\n",
    "    acc = round((prec + recall) / 2, 4)\n",
    "\n",
    "    # --- Benchmark inference ---\n",
    "    dummy = torch.randn(1, 3, imgsz, imgsz).to(device)\n",
    "    warmup, runs = 5, 20\n",
    "    for _ in range(warmup):\n",
    "        _ = model.model(dummy)\n",
    "    t_inf = time.time()\n",
    "    for _ in range(runs):\n",
    "        _ = model.model(dummy)\n",
    "    torch.cuda.synchronize() if device == \"cuda\" else None\n",
    "    infer_time = (time.time() - t_inf) / runs\n",
    "    fps = round(1.0 / infer_time, 2)\n",
    "\n",
    "    return {\n",
    "        \"Model Name\": model_name,\n",
    "        \"Backend\": \"PyTorch\",\n",
    "        \"Eval Type\": eval_type,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-score\": f1,\n",
    "        \"Accuracy\": acc,\n",
    "        \"mAP@50\": map50,\n",
    "        \"mAP@50-95\": map95,\n",
    "        \"Eval Time (s)\": round(eval_time, 2),\n",
    "        \"Infer Time (s/img)\": round(infer_time, 4),\n",
    "        \"FPS\": fps\n",
    "    }\n",
    "\n",
    "# H√†m benchmark ONNX\n",
    "def evaluate_onnx(onnx_path, data_yaml, eval_type=\"Self\", device=\"cpu\", imgsz=640, batch=4, num_threads=8, model_name=None):\n",
    "    print(f\"\\nüìà [{eval_type}] Evaluating (ONNX): {os.path.basename(onnx_path)} on {data_yaml}\")\n",
    "\n",
    "    # --- Setup threading & provider ---\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(num_threads)\n",
    "    os.environ[\"MKL_NUM_THREADS\"] = str(num_threads)\n",
    "    providers = [\"CPUExecutionProvider\"] if device == \"cpu\" else [\"CUDAExecutionProvider\"]\n",
    "\n",
    "    # --- Load YOLO ONNX for evaluation ---\n",
    "    model = YOLO(onnx_path, task=\"detect\")\n",
    "\n",
    "    # --- Validation phase ---\n",
    "    t0 = time.time()\n",
    "    metrics = model.val(data=data_yaml, split=\"val\", imgsz=imgsz, batch=batch, device=device, verbose=False)\n",
    "    eval_time = time.time() - t0\n",
    "\n",
    "    # --- Extract metrics ---\n",
    "    prec = round(metrics.results_dict.get(\"metrics/precision(B)\", 0), 4)\n",
    "    recall = round(metrics.results_dict.get(\"metrics/recall(B)\", 0), 4)\n",
    "    map50 = round(metrics.results_dict.get(\"metrics/mAP50(B)\", 0), 4)\n",
    "    map5095 = round(metrics.results_dict.get(\"metrics/mAP50-95(B)\", 0), 4)\n",
    "    f1 = round(2 * (prec * recall) / (prec + recall + 1e-9), 4)\n",
    "    acc = round((prec + recall) / 2, 4)\n",
    "\n",
    "    # --- Benchmark inference using onnxruntime ---\n",
    "    sess_options = ort.SessionOptions()\n",
    "    sess_options.intra_op_num_threads = num_threads\n",
    "    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    session = ort.InferenceSession(onnx_path, sess_options, providers=providers)\n",
    "    \n",
    "\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    dummy = np.random.randn(1, 3, imgsz, imgsz).astype(np.float32)\n",
    "\n",
    "    # Warmup + benchmark\n",
    "    warmup, runs = 5, 20\n",
    "    for _ in range(warmup):\n",
    "        _ = session.run(None, {input_name: dummy})\n",
    "    t_inf = time.time()\n",
    "    for _ in range(runs):\n",
    "        _ = session.run(None, {input_name: dummy})\n",
    "    infer_time = (time.time() - t_inf) / runs\n",
    "    fps = round(1.0 / infer_time, 2)\n",
    "\n",
    "    return {\n",
    "        \"Model Name\": model_name,\n",
    "        \"Backend\": \"ONNX\",\n",
    "        \"Eval Type\": eval_type,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-score\": f1,\n",
    "        \"Accuracy\": acc,\n",
    "        \"mAP@50\": map50,\n",
    "        \"mAP@50-95\": map5095,\n",
    "        \"Eval Time (s)\": round(eval_time, 2),\n",
    "        \"Infer Time (s/img)\": round(infer_time, 4),\n",
    "        \"FPS\": fps\n",
    "\n",
    "    }\n",
    "\n",
    "# H√†m benchmark OpenVINO\n",
    "def evaluate_openvino(openvino_path, data_yaml, eval_type=\"Self\", device=\"cpu\", imgsz=640, batch=1, num_threads=4, model_name=None):\n",
    "    \"\"\"\n",
    "    Benchmark OpenVINO model (optimized for Raspberry Pi 5 ARM64)\n",
    "    \n",
    "    Args:\n",
    "        openvino_path: Path to folder containing .xml, .bin files\n",
    "        data_yaml: Path to dataset YAML\n",
    "        eval_type: \"Self\" or \"Cross\"\n",
    "        device: \"cpu\" only (OpenVINO on Pi doesn't support GPU)\n",
    "        imgsz: Input image size\n",
    "        batch: Batch size (recommend 1 for Pi)\n",
    "        num_threads: Number of CPU threads (4 for Pi 5)\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìà [{eval_type}] Evaluating (OpenVINO): {os.path.basename(openvino_path)} on {data_yaml}\")\n",
    "    \n",
    "    try:\n",
    "        from openvino.runtime import Core\n",
    "    except ImportError:\n",
    "        print(\"‚ùå OpenVINO not installed! Install: pip install openvino\")\n",
    "        return None\n",
    "\n",
    "    # --- Setup threading ---\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(num_threads)\n",
    "    os.environ[\"MKL_NUM_THREADS\"] = str(num_threads)\n",
    "    \n",
    "    # --- Find .xml file in folder ---\n",
    "    openvino_folder = Path(openvino_path)\n",
    "    xml_files = list(openvino_folder.glob(\"*.xml\"))\n",
    "    \n",
    "    if not xml_files:\n",
    "        print(f\"‚ùå No .xml file found in {openvino_path}\")\n",
    "        return None\n",
    "    \n",
    "    model_xml = str(xml_files[0])\n",
    "    print(f\"üì¶ Found model: {model_xml}\")\n",
    "    \n",
    "    # --- Load YOLO OpenVINO for evaluation ---\n",
    "    try:\n",
    "        model = YOLO(openvino_path, task=\"detect\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è YOLO direct load failed: {e}\")\n",
    "        print(\"üìù Will use OpenVINO Core directly for inference benchmark\")\n",
    "        model = None\n",
    "    \n",
    "    # --- Validation phase (if YOLO wrapper works) ---\n",
    "    eval_time = 0\n",
    "    prec = recall = map50 = map5095 = f1 = acc = 0\n",
    "    \n",
    "    if model is not None:\n",
    "        try:\n",
    "            t0 = time.time()\n",
    "            metrics = model.val(\n",
    "                data=data_yaml, \n",
    "                split=\"val\", \n",
    "                imgsz=imgsz, \n",
    "                batch=batch, \n",
    "                device=device, \n",
    "                verbose=False\n",
    "            )\n",
    "            eval_time = time.time() - t0\n",
    "            \n",
    "            # --- Extract metrics ---\n",
    "            prec = round(metrics.results_dict.get(\"metrics/precision(B)\", 0), 4)\n",
    "            recall = round(metrics.results_dict.get(\"metrics/recall(B)\", 0), 4)\n",
    "            map50 = round(metrics.results_dict.get(\"metrics/mAP50(B)\", 0), 4)\n",
    "            map5095 = round(metrics.results_dict.get(\"metrics/mAP50-95(B)\", 0), 4)\n",
    "            f1 = round(2 * (prec * recall) / (prec + recall + 1e-9), 4)\n",
    "            acc = round((prec + recall) / 2, 4)\n",
    "            print(f\"‚úÖ Validation completed: mAP@50={map50}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Validation failed: {e}\")\n",
    "            print(\"üìù Skipping validation, will only benchmark inference speed\")\n",
    "    \n",
    "    # --- Benchmark inference using OpenVINO Core ---\n",
    "    print(\"üîß Loading OpenVINO model for inference benchmark...\")\n",
    "    ie = Core()\n",
    "    \n",
    "    # ARM optimization for Pi 5\n",
    "    config = {\n",
    "        \"PERFORMANCE_HINT\": \"LATENCY\",  # Optimize for low latency\n",
    "        \"NUM_STREAMS\": \"1\",\n",
    "        \"INFERENCE_NUM_THREADS\": str(num_threads),\n",
    "        \"ENABLE_CPU_PINNING\": \"YES\",\n",
    "        \"INFERENCE_PRECISION_HINT\": \"f32\"  # Pi 5 doesn't have good fp16 support\n",
    "    }\n",
    "    \n",
    "    compiled_model = ie.compile_model(model=model_xml, device_name=\"CPU\", config=config)\n",
    "    infer_request = compiled_model.create_infer_request()\n",
    "    \n",
    "    # Get input tensor info\n",
    "    input_layer = compiled_model.input(0)\n",
    "    input_shape = input_layer.shape\n",
    "    print(f\"üìê Model input shape: {input_shape}\")\n",
    "    \n",
    "    # Prepare dummy input\n",
    "    dummy = np.random.randn(1, 3, imgsz, imgsz).astype(np.float32)\n",
    "    \n",
    "    # Warmup\n",
    "    warmup, runs = 5, 20\n",
    "    print(f\"üî• Warming up {warmup} iterations...\")\n",
    "    for _ in range(warmup):\n",
    "        infer_request.infer({0: dummy})\n",
    "    \n",
    "    # Benchmark\n",
    "    print(f\"‚è±Ô∏è Benchmarking {runs} iterations...\")\n",
    "    t_inf = time.time()\n",
    "    for _ in range(runs):\n",
    "        infer_request.infer({0: dummy})\n",
    "    infer_time = (time.time() - t_inf) / runs\n",
    "    fps = round(1.0 / infer_time, 2)\n",
    "    print(f\"‚úÖ Inference time: {infer_time*1000:.2f} ms/image\")\n",
    "    \n",
    "    return {\n",
    "        \"Model Name\": model_name,\n",
    "        \"Backend\": \"OpenVINO\",\n",
    "        \"Eval Type\": eval_type,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-score\": f1,\n",
    "        \"Accuracy\": acc,\n",
    "        \"mAP@50\": map50,\n",
    "        \"mAP@50-95\": map5095,\n",
    "        \"Eval Time (s)\": round(eval_time, 2),\n",
    "        \"Infer Time (s/img)\": round(infer_time, 4),\n",
    "        \"FPS\": fps\n",
    "    }\n",
    "\n",
    "def evaluate_ncnn(ncnn_folder, data_yaml, eval_type=\"Self\", imgsz=640, num_threads=4, model_name=None):\n",
    "    \"\"\"\n",
    "    Benchmark NCNN model (inference b·∫±ng runtime thu·∫ßn NCNN, val d√πng YOLO wrapper)\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìà [{eval_type}] Evaluating (NCNN): {os.path.basename(ncnn_folder)} on {data_yaml}\")\n",
    "\n",
    "    # --- Load model YOLO (wrapper) ƒë·ªÉ t√≠nh mAP ---\n",
    "    try:\n",
    "        model = YOLO(ncnn_folder, task=\"detect\")\n",
    "        t0 = time.time()\n",
    "        metrics = model.val(data=data_yaml, imgsz=imgsz, split=\"val\", batch=1, device=\"cpu\", verbose=False)\n",
    "        eval_time = time.time() - t0\n",
    "\n",
    "        prec = round(metrics.results_dict.get(\"metrics/precision(B)\", 0), 4)\n",
    "        recall = round(metrics.results_dict.get(\"metrics/recall(B)\", 0), 4)\n",
    "        map50 = round(metrics.results_dict.get(\"metrics/mAP50(B)\", 0), 4)\n",
    "        map5095 = round(metrics.results_dict.get(\"metrics/mAP50-95(B)\", 0), 4)\n",
    "        f1 = round(2 * (prec * recall) / (prec + recall + 1e-9), 4)\n",
    "        acc = round((prec + recall) / 2, 4)\n",
    "        print(f\"‚úÖ Validation done: mAP@50={map50}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Validation failed with YOLO wrapper: {e}\")\n",
    "        prec = recall = map50 = map5095 = f1 = acc = 0\n",
    "        eval_time = 0\n",
    "\n",
    "    # --- Benchmark inference b·∫±ng NCNN thu·∫ßn ---\n",
    "    print(\"üîß Benchmarking raw NCNN inference...\")\n",
    "    param_path = Path(ncnn_folder) / \"model.ncnn.param\"\n",
    "    bin_path = Path(ncnn_folder) / \"model.ncnn.bin\"\n",
    "    if not param_path.exists() or not bin_path.exists():\n",
    "        print(f\"‚ùå Missing NCNN files in {ncnn_folder}\")\n",
    "        return None\n",
    "\n",
    "    net = ncnn.Net()\n",
    "    net.opt.use_vulkan_compute = False\n",
    "    net.opt.num_threads = num_threads\n",
    "    net.load_param(str(param_path))\n",
    "    net.load_model(str(bin_path))\n",
    "\n",
    "    # --- Dummy input ---\n",
    "    dummy = np.random.randn(1, 3, imgsz, imgsz).astype(np.float32)\n",
    "\n",
    "    # --- Warmup ---\n",
    "    warmup, runs = 5, 20\n",
    "    for _ in range(warmup):\n",
    "        ex = net.create_extractor()\n",
    "        ex.input(\"in0\", ncnn.Mat(dummy))\n",
    "        _, _ = ex.extract(\"out0\")  \n",
    "\n",
    "    # --- Benchmark ---\n",
    "    t_inf = time.time()\n",
    "    for _ in range(runs):\n",
    "        ex = net.create_extractor()\n",
    "        ex.input(\"in0\", ncnn.Mat(dummy))\n",
    "        _, _ = ex.extract(\"out0\")\n",
    "    infer_time = (time.time() - t_inf) / runs\n",
    "    fps = round(1.0 / infer_time, 2)\n",
    "\n",
    "    print(f\"‚úÖ NCNN Inference time: {infer_time*1000:.2f} ms/image\")\n",
    "\n",
    "    return {\n",
    "        \"Model Name\": model_name,\n",
    "        \"Backend\": \"NCNN\",\n",
    "        \"Eval Type\": eval_type,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-score\": f1,\n",
    "        \"Accuracy\": acc,\n",
    "        \"mAP@50\": map50,\n",
    "        \"mAP@50-95\": map5095,\n",
    "        \"Eval Time (s)\": round(eval_time, 2),\n",
    "        \"Infer Time (s/img)\": round(infer_time, 4),\n",
    "        \"FPS\": fps\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1927367c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Running PyTorch Benchmark for YOLOv5n\n",
      "\n",
      "üìà [YOLOv5n] Evaluating (PyTorch): yolo5.pt on /home/pi5/TrafficSign/Dataset/Detect/data.yaml\n",
      "Ultralytics 8.3.227 üöÄ Python-3.13.5 torch-2.9.0+cpu CPU (aarch64)\n",
      "YOLOv5n summary (fused): 84 layers, 2,503,139 parameters, 0 gradients, 7.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 75.6¬±6.3 MB/s, size: 1003.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/pi5/TrafficSign/Dataset/Detect/labels/val.cache... 1082 images, 31 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1082/1082 1.7Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 271/271 0.5it/s 9:10<2.0s\n",
      "                   all       1082       2783      0.932      0.827      0.909       0.66\n",
      "Speed: 2.8ms preprocess, 447.5ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1m/home/pi5/TrafficSign/runs/detect/val23\u001b[0m\n",
      "\n",
      "üöÄ Running PyTorch Benchmark for YOLOv8n\n",
      "\n",
      "üìà [YOLOv8n] Evaluating (PyTorch): yolo8.pt on /home/pi5/TrafficSign/Dataset/Detect/data.yaml\n",
      "Ultralytics 8.3.227 üöÄ Python-3.13.5 torch-2.9.0+cpu CPU (aarch64)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 675.3¬±1187.4 MB/s, size: 1180.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/pi5/TrafficSign/Dataset/Detect/labels/val.cache... 1082 images, 31 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1082/1082 1.9Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 271/271 0.5it/s 9:45<2.2s\n",
      "                   all       1082       2783      0.926       0.83      0.913      0.665\n",
      "Speed: 3.8ms preprocess, 478.6ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1m/home/pi5/TrafficSign/runs/detect/val24\u001b[0m\n",
      "\n",
      "üöÄ Running PyTorch Benchmark for YOLO11n\n",
      "\n",
      "üìà [YOLO11n] Evaluating (PyTorch): yolo11.pt on /home/pi5/TrafficSign/Dataset/Detect/data.yaml\n",
      "Ultralytics 8.3.227 üöÄ Python-3.13.5 torch-2.9.0+cpu CPU (aarch64)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 702.3¬±1227.7 MB/s, size: 1216.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/pi5/TrafficSign/Dataset/Detect/labels/val.cache... 1082 images, 31 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1082/1082 1.9Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 271/271 0.5it/s 9:27<2.1s\n",
      "                   all       1082       2783      0.933      0.835      0.913      0.668\n",
      "Speed: 3.7ms preprocess, 458.8ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1m/home/pi5/TrafficSign/runs/detect/val25\u001b[0m\n",
      "\n",
      "üöÄ Running ONNX Benchmark for YOLOv5n\n",
      "\n",
      "üìà [YOLOv5n] Evaluating (ONNX): yolov5.onnx on /home/pi5/TrafficSign/Dataset/Detect/data.yaml\n",
      "Ultralytics 8.3.227 üöÄ Python-3.13.5 torch-2.9.0+cpu CPU (aarch64)\n",
      "Loading /home/pi5/TrafficSign/convert/model/yolov5/yolov5.onnx for ONNX Runtime inference...\n",
      "Using ONNX Runtime 1.23.2 CPUExecutionProvider\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 78.8¬±14.1 MB/s, size: 1204.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/pi5/TrafficSign/Dataset/Detect/labels/val.cache... 1082 images, 31 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1082/1082 1.9Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 271/271 0.9it/s 5:07<1.1s\n",
      "                   all       1082       2783      0.932      0.827      0.909       0.66\n",
      "Speed: 2.3ms preprocess, 215.6ms inference, 0.0ms loss, 1.8ms postprocess per image\n",
      "Results saved to \u001b[1m/home/pi5/TrafficSign/runs/detect/val26\u001b[0m\n",
      "\n",
      "üöÄ Running ONNX Benchmark for YOLOv8n\n",
      "\n",
      "üìà [YOLOv8n] Evaluating (ONNX): yolov8.onnx on /home/pi5/TrafficSign/Dataset/Detect/data.yaml\n",
      "Ultralytics 8.3.227 üöÄ Python-3.13.5 torch-2.9.0+cpu CPU (aarch64)\n",
      "Loading /home/pi5/TrafficSign/convert/model/yolov8/yolov8.onnx for ONNX Runtime inference...\n",
      "Using ONNX Runtime 1.23.2 CPUExecutionProvider\n",
      "Setting batch=1 input of shape (1, 3, 640, 640)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 77.4¬±5.4 MB/s, size: 1235.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/pi5/TrafficSign/Dataset/Detect/labels/val.cache... 1082 images, 31 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1082/1082 1.9Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1082/1082 4.2it/s 4:18<0.2s\n",
      "                   all       1082       2783      0.914      0.839      0.911      0.664\n",
      "Speed: 2.5ms preprocess, 176.3ms inference, 0.0ms loss, 2.4ms postprocess per image\n",
      "Results saved to \u001b[1m/home/pi5/TrafficSign/runs/detect/val27\u001b[0m\n",
      "\n",
      "üöÄ Running ONNX Benchmark for YOLO11n\n",
      "\n",
      "üìà [YOLO11n] Evaluating (ONNX): yolov11.onnx on /home/pi5/TrafficSign/Dataset/Detect/data.yaml\n",
      "Ultralytics 8.3.227 üöÄ Python-3.13.5 torch-2.9.0+cpu CPU (aarch64)\n",
      "Loading /home/pi5/TrafficSign/convert/model/yolov11/yolov11.onnx for ONNX Runtime inference...\n",
      "Using ONNX Runtime 1.23.2 CPUExecutionProvider\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 77.4¬±4.5 MB/s, size: 1030.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/pi5/TrafficSign/Dataset/Detect/labels/val.cache... 1082 images, 31 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1082/1082 2.1Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 271/271 0.8it/s 5:41<1.2s\n",
      "                   all       1082       2783      0.933      0.835      0.913      0.668\n",
      "Speed: 2.4ms preprocess, 250.9ms inference, 0.0ms loss, 1.7ms postprocess per image\n",
      "Results saved to \u001b[1m/home/pi5/TrafficSign/runs/detect/val28\u001b[0m\n",
      "\n",
      "üöÄ Running OpenVINO Benchmark for YOLOv5n\n",
      "\n",
      "üìà [YOLOv5n] Evaluating (OpenVINO): yolo5_openvino_model on /home/pi5/TrafficSign/Dataset/Detect/data.yaml\n",
      "üì¶ Found model: /home/pi5/TrafficSign/convert/model/yolov5/yolo5_openvino_model/yolo5.xml\n",
      "Ultralytics 8.3.227 üöÄ Python-3.13.5 torch-2.9.0+cpu CPU (aarch64)\n",
      "Loading /home/pi5/TrafficSign/convert/model/yolov5/yolo5_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference on CPU...\n",
      "Setting batch=1 input of shape (1, 3, 640, 640)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 133.9¬±79.8 MB/s, size: 1039.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/pi5/TrafficSign/Dataset/Detect/labels/val.cache... 1082 images, 31 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1082/1082 2.2Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1082/1082 7.0it/s 2:35<0.1s\n",
      "                   all       1082       2783      0.926      0.825      0.905      0.655\n",
      "Speed: 2.2ms preprocess, 80.5ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1m/home/pi5/TrafficSign/runs/detect/val29\u001b[0m\n",
      "‚úÖ Validation completed: mAP@50=0.9053\n",
      "üîß Loading OpenVINO model for inference benchmark...\n",
      "üìê Model input shape: [1,3,640,640]\n",
      "üî• Warming up 5 iterations...\n",
      "‚è±Ô∏è Benchmarking 20 iterations...\n",
      "‚úÖ Inference time: 166.25 ms/image\n",
      "\n",
      "üöÄ Running OpenVINO Benchmark for YOLOv8n\n",
      "\n",
      "üìà [YOLOv8n] Evaluating (OpenVINO): yolo8_openvino_model on /home/pi5/TrafficSign/Dataset/Detect/data.yaml\n",
      "üì¶ Found model: /home/pi5/TrafficSign/convert/model/yolov8/yolo8_openvino_model/yolo8.xml\n",
      "Ultralytics 8.3.227 üöÄ Python-3.13.5 torch-2.9.0+cpu CPU (aarch64)\n",
      "Loading /home/pi5/TrafficSign/convert/model/yolov8/yolo8_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference on CPU...\n",
      "Setting batch=1 input of shape (1, 3, 640, 640)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 92.5¬±22.8 MB/s, size: 1138.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/pi5/TrafficSign/Dataset/Detect/labels/val.cache... 1082 images, 31 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1082/1082 1.9Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1082/1082 7.9it/s 2:17<0.1s\n",
      "                   all       1082       2783      0.915      0.841      0.911      0.658\n",
      "Speed: 2.1ms preprocess, 66.8ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1m/home/pi5/TrafficSign/runs/detect/val30\u001b[0m\n",
      "‚úÖ Validation completed: mAP@50=0.9108\n",
      "üîß Loading OpenVINO model for inference benchmark...\n",
      "üìê Model input shape: [1,3,640,640]\n",
      "üî• Warming up 5 iterations...\n",
      "‚è±Ô∏è Benchmarking 20 iterations...\n",
      "‚úÖ Inference time: 152.99 ms/image\n",
      "\n",
      "üöÄ Running OpenVINO Benchmark for YOLO11n\n",
      "\n",
      "üìà [YOLO11n] Evaluating (OpenVINO): yolo11_openvino_model on /home/pi5/TrafficSign/Dataset/Detect/data.yaml\n",
      "üì¶ Found model: /home/pi5/TrafficSign/convert/model/yolov11/yolo11_openvino_model/yolo11.xml\n",
      "Ultralytics 8.3.227 üöÄ Python-3.13.5 torch-2.9.0+cpu CPU (aarch64)\n",
      "Loading /home/pi5/TrafficSign/convert/model/yolov11/yolo11_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference on CPU...\n",
      "Setting batch=1 input of shape (1, 3, 640, 640)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 77.6¬±5.5 MB/s, size: 1149.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/pi5/TrafficSign/Dataset/Detect/labels/val.cache... 1082 images, 31 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1082/1082 2.1Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1082/1082 7.7it/s 2:21<0.1s\n",
      "                   all       1082       2783      0.929      0.835      0.911      0.665\n",
      "Speed: 2.3ms preprocess, 69.5ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1m/home/pi5/TrafficSign/runs/detect/val31\u001b[0m\n",
      "‚úÖ Validation completed: mAP@50=0.9109\n",
      "üîß Loading OpenVINO model for inference benchmark...\n",
      "üìê Model input shape: [1,3,640,640]\n",
      "üî• Warming up 5 iterations...\n",
      "‚è±Ô∏è Benchmarking 20 iterations...\n",
      "‚úÖ Inference time: 179.69 ms/image\n",
      "\n",
      "üöÄ Running NCNN Benchmark for YOLOv5n\n",
      "\n",
      "üìà [YOLOv5n] Evaluating (NCNN): yolo5_ncnn_model on /home/pi5/TrafficSign/Dataset/Detect/data.yaml\n",
      "Ultralytics 8.3.227 üöÄ Python-3.13.5 torch-2.9.0+cpu CPU (aarch64)\n",
      "Loading /home/pi5/TrafficSign/convert/model/yolov5/yolo5_ncnn_model for NCNN inference...\n",
      "Setting batch=1 input of shape (1, 3, 640, 640)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 78.8¬±10.4 MB/s, size: 1032.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/pi5/TrafficSign/Dataset/Detect/labels/val.cache... 1082 images, 31 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1082/1082 1.5Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1082/1082 6.9it/s 2:37<0.1s\n",
      "                   all       1082       2783      0.923      0.827      0.906      0.655\n",
      "Speed: 2.3ms preprocess, 78.0ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1m/home/pi5/TrafficSign/runs/detect/val32\u001b[0m\n",
      "‚úÖ Validation done: mAP@50=0.9063\n",
      "üîß Benchmarking raw NCNN inference...\n",
      "‚úÖ NCNN Inference time: 77.36 ms/image\n",
      "\n",
      "üöÄ Running NCNN Benchmark for YOLOv8n\n",
      "\n",
      "üìà [YOLOv8n] Evaluating (NCNN): yolo8_ncnn_model on /home/pi5/TrafficSign/Dataset/Detect/data.yaml\n",
      "Ultralytics 8.3.227 üöÄ Python-3.13.5 torch-2.9.0+cpu CPU (aarch64)\n",
      "Loading /home/pi5/TrafficSign/convert/model/yolov8/yolo8_ncnn_model for NCNN inference...\n",
      "Setting batch=1 input of shape (1, 3, 640, 640)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 75.6¬±7.6 MB/s, size: 1033.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/pi5/TrafficSign/Dataset/Detect/labels/val.cache... 1082 images, 31 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1082/1082 2.0Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1082/1082 6.7it/s 2:42<0.1s\n",
      "                   all       1082       2783      0.914      0.841      0.911      0.658\n",
      "Speed: 2.3ms preprocess, 87.6ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1m/home/pi5/TrafficSign/runs/detect/val33\u001b[0m\n",
      "‚úÖ Validation done: mAP@50=0.9109\n",
      "üîß Benchmarking raw NCNN inference...\n",
      "‚úÖ NCNN Inference time: 90.69 ms/image\n",
      "\n",
      "üöÄ Running NCNN Benchmark for YOLO11n\n",
      "\n",
      "üìà [YOLO11n] Evaluating (NCNN): yolo11_ncnn_model on /home/pi5/TrafficSign/Dataset/Detect/data.yaml\n",
      "Ultralytics 8.3.227 üöÄ Python-3.13.5 torch-2.9.0+cpu CPU (aarch64)\n",
      "Loading /home/pi5/TrafficSign/convert/model/yolov11/yolo11_ncnn_model for NCNN inference...\n",
      "Setting batch=1 input of shape (1, 3, 640, 640)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 77.2¬±5.8 MB/s, size: 1042.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/pi5/TrafficSign/Dataset/Detect/labels/val.cache... 1082 images, 31 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1082/1082 1.8Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1082/1082 6.6it/s 2:44<0.1s\n",
      "                   all       1082       2783      0.929      0.835      0.911      0.665\n",
      "Speed: 2.7ms preprocess, 88.9ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1m/home/pi5/TrafficSign/runs/detect/val34\u001b[0m\n",
      "‚úÖ Validation done: mAP@50=0.911\n",
      "üîß Benchmarking raw NCNN inference...\n",
      "‚úÖ NCNN Inference time: 86.27 ms/image\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '/Results'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     60\u001b[39m df = pd.DataFrame(results)\n\u001b[32m     61\u001b[39m save_csv = \u001b[33m\"\u001b[39m\u001b[33m/Results/fair_benchmark_results_single.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ Saved benchmark results to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_csv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TrafficSign/tsr/lib/python3.13/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TrafficSign/tsr/lib/python3.13/site-packages/pandas/core/generic.py:3989\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3978\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3980\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3981\u001b[39m     frame=df,\n\u001b[32m   3982\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3986\u001b[39m     decimal=decimal,\n\u001b[32m   3987\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3989\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3990\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3991\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3992\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3993\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3994\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3995\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3996\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4006\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TrafficSign/tsr/lib/python3.13/site-packages/pandas/io/formats/format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TrafficSign/tsr/lib/python3.13/site-packages/pandas/io/formats/csvs.py:251\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TrafficSign/tsr/lib/python3.13/site-packages/pandas/io/common.py:749\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression != \u001b[33m\"\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TrafficSign/tsr/lib/python3.13/site-packages/pandas/io/common.py:616\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    614\u001b[39m parent = Path(path).parent\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: '/Results'"
     ]
    }
   ],
   "source": [
    "# --- ƒê∆∞·ªùng d·∫´n dataset duy nh·∫•t ---\n",
    "dataset1 = \"/home/pi5/TrafficSign/Dataset/Detect/data.yaml\"\n",
    "\n",
    "# --- ƒê·ªãnh nghƒ©a model paths ---\n",
    "models_pt = {\n",
    "    \"YOLOv5n\": \"/home/pi5/TrafficSign/WeightDetection/yolo5.pt\",\n",
    "    \"YOLOv8n\": \"/home/pi5/TrafficSign/WeightDetection/yolo8.pt\",\n",
    "    \"YOLO11n\": \"/home/pi5/TrafficSign/WeightDetection/yolo11.pt\",\n",
    "}\n",
    "\n",
    "models_onnx = {\n",
    "    \"YOLOv5n\": \"/home/pi5/TrafficSign/convert/model/yolov5/yolov5.onnx\",\n",
    "    \"YOLOv8n\": \"/home/pi5/TrafficSign/convert/model/yolov8/yolov8.onnx\",\n",
    "    \"YOLO11n\": \"/home/pi5/TrafficSign/convert/model/yolov11/yolov11.onnx\",\n",
    "}\n",
    "\n",
    "models_openvino = {\n",
    "    \"YOLOv5n\": \"/home/pi5/TrafficSign/convert/model/yolov5/yolo5_openvino_model\",\n",
    "    \"YOLOv8n\": \"/home/pi5/TrafficSign/convert/model/yolov8/yolo8_openvino_model\",\n",
    "    \"YOLO11n\": \"/home/pi5/TrafficSign/convert/model/yolov11/yolo11_openvino_model\",\n",
    "}\n",
    "\n",
    "models_ncnn = {\n",
    "    \"YOLOv5n\": \"/home/pi5/TrafficSign/convert/model/yolov5/yolo5_ncnn_model\",\n",
    "    \"YOLOv8n\": \"/home/pi5/TrafficSign/convert/model/yolov8/yolo8_ncnn_model\",\n",
    "    \"YOLO11n\": \"/home/pi5/TrafficSign/convert/model/yolov11/yolo11_ncnn_model\",\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# --- Benchmark PyTorch ---\n",
    "for model_name, model_path in models_pt.items():\n",
    "    print(f\"\\nüöÄ Running PyTorch Benchmark for {model_name}\")\n",
    "    results.append(\n",
    "        evaluate_model(model_path, dataset1, device=\"cpu\", eval_type=f\"{model_name}\", model_name=model_name)\n",
    "    )\n",
    "\n",
    "# --- Benchmark ONNX ---\n",
    "for model_name, model_path in models_onnx.items():\n",
    "    print(f\"\\nüöÄ Running ONNX Benchmark for {model_name}\")\n",
    "    results.append(\n",
    "        evaluate_onnx(model_path, dataset1, eval_type=f\"{model_name}\", device=\"cpu\", model_name=model_name)\n",
    "    )\n",
    "\n",
    "# --- Benchmark OpenVINO ---\n",
    "for model_name, model_path in models_openvino.items():\n",
    "    print(f\"\\nüöÄ Running OpenVINO Benchmark for {model_name}\")\n",
    "    result = evaluate_openvino(model_path, dataset1, eval_type=f\"{model_name}\", device=\"cpu\", batch=1, num_threads=4, model_name=model_name)\n",
    "    if result:\n",
    "        results.append(result)\n",
    "\n",
    "# --- Benchmark NCNN ---\n",
    "for model_name, model_path in models_ncnn.items():\n",
    "    print(f\"\\nüöÄ Running NCNN Benchmark for {model_name}\")\n",
    "    result = evaluate_ncnn(model_path, dataset1, eval_type=f\"{model_name}\", imgsz=640, num_threads=4, model_name=model_name)\n",
    "    if result:\n",
    "        results.append(result)\n",
    "\n",
    "# --- L∆∞u k·∫øt qu·∫£ ---\n",
    "df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ec8ad5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Saved benchmark results to: /home/pi5/TrafficSign/Results/fair_benchmark_results_single.csv\n"
     ]
    }
   ],
   "source": [
    "save_csv = \"/home/pi5/TrafficSign/Results/fair_benchmark_results_single.csv\"\n",
    "df.to_csv(save_csv, index=False)\n",
    "print(f\"\\n‚úÖ Saved benchmark results to: {save_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18562a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Backend</th>\n",
       "      <th>Eval Type</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>mAP@50</th>\n",
       "      <th>mAP@50-95</th>\n",
       "      <th>Eval Time (s)</th>\n",
       "      <th>Infer Time (s/img)</th>\n",
       "      <th>FPS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YOLOv5n</td>\n",
       "      <td>PyTorch</td>\n",
       "      <td>YOLOv5n</td>\n",
       "      <td>0.9322</td>\n",
       "      <td>0.8268</td>\n",
       "      <td>0.8763</td>\n",
       "      <td>0.8795</td>\n",
       "      <td>0.9090</td>\n",
       "      <td>0.6600</td>\n",
       "      <td>552.35</td>\n",
       "      <td>0.3965</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YOLOv8n</td>\n",
       "      <td>PyTorch</td>\n",
       "      <td>YOLOv8n</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>0.8300</td>\n",
       "      <td>0.8753</td>\n",
       "      <td>0.8780</td>\n",
       "      <td>0.9127</td>\n",
       "      <td>0.6652</td>\n",
       "      <td>587.73</td>\n",
       "      <td>0.4177</td>\n",
       "      <td>2.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YOLO11n</td>\n",
       "      <td>PyTorch</td>\n",
       "      <td>YOLO11n</td>\n",
       "      <td>0.9325</td>\n",
       "      <td>0.8345</td>\n",
       "      <td>0.8808</td>\n",
       "      <td>0.8835</td>\n",
       "      <td>0.9134</td>\n",
       "      <td>0.6680</td>\n",
       "      <td>571.83</td>\n",
       "      <td>0.4034</td>\n",
       "      <td>2.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YOLOv5n</td>\n",
       "      <td>ONNX</td>\n",
       "      <td>YOLOv5n</td>\n",
       "      <td>0.9322</td>\n",
       "      <td>0.8268</td>\n",
       "      <td>0.8763</td>\n",
       "      <td>0.8795</td>\n",
       "      <td>0.9090</td>\n",
       "      <td>0.6600</td>\n",
       "      <td>309.51</td>\n",
       "      <td>0.3744</td>\n",
       "      <td>2.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YOLOv8n</td>\n",
       "      <td>ONNX</td>\n",
       "      <td>YOLOv8n</td>\n",
       "      <td>0.9139</td>\n",
       "      <td>0.8394</td>\n",
       "      <td>0.8751</td>\n",
       "      <td>0.8767</td>\n",
       "      <td>0.9109</td>\n",
       "      <td>0.6636</td>\n",
       "      <td>260.83</td>\n",
       "      <td>0.4157</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>YOLO11n</td>\n",
       "      <td>ONNX</td>\n",
       "      <td>YOLO11n</td>\n",
       "      <td>0.9325</td>\n",
       "      <td>0.8345</td>\n",
       "      <td>0.8808</td>\n",
       "      <td>0.8835</td>\n",
       "      <td>0.9134</td>\n",
       "      <td>0.6680</td>\n",
       "      <td>343.78</td>\n",
       "      <td>0.4665</td>\n",
       "      <td>2.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>YOLOv5n</td>\n",
       "      <td>OpenVINO</td>\n",
       "      <td>YOLOv5n</td>\n",
       "      <td>0.9258</td>\n",
       "      <td>0.8246</td>\n",
       "      <td>0.8723</td>\n",
       "      <td>0.8752</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>0.6555</td>\n",
       "      <td>158.73</td>\n",
       "      <td>0.1662</td>\n",
       "      <td>6.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>YOLOv8n</td>\n",
       "      <td>OpenVINO</td>\n",
       "      <td>YOLOv8n</td>\n",
       "      <td>0.9147</td>\n",
       "      <td>0.8412</td>\n",
       "      <td>0.8764</td>\n",
       "      <td>0.8780</td>\n",
       "      <td>0.9108</td>\n",
       "      <td>0.6575</td>\n",
       "      <td>139.67</td>\n",
       "      <td>0.1530</td>\n",
       "      <td>6.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>YOLO11n</td>\n",
       "      <td>OpenVINO</td>\n",
       "      <td>YOLO11n</td>\n",
       "      <td>0.9289</td>\n",
       "      <td>0.8355</td>\n",
       "      <td>0.8797</td>\n",
       "      <td>0.8822</td>\n",
       "      <td>0.9109</td>\n",
       "      <td>0.6649</td>\n",
       "      <td>143.87</td>\n",
       "      <td>0.1797</td>\n",
       "      <td>5.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>YOLOv5n</td>\n",
       "      <td>NCNN</td>\n",
       "      <td>YOLOv5n</td>\n",
       "      <td>0.9230</td>\n",
       "      <td>0.8268</td>\n",
       "      <td>0.8723</td>\n",
       "      <td>0.8749</td>\n",
       "      <td>0.9063</td>\n",
       "      <td>0.6550</td>\n",
       "      <td>159.67</td>\n",
       "      <td>0.0774</td>\n",
       "      <td>12.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>YOLOv8n</td>\n",
       "      <td>NCNN</td>\n",
       "      <td>YOLOv8n</td>\n",
       "      <td>0.9141</td>\n",
       "      <td>0.8408</td>\n",
       "      <td>0.8759</td>\n",
       "      <td>0.8775</td>\n",
       "      <td>0.9109</td>\n",
       "      <td>0.6579</td>\n",
       "      <td>164.79</td>\n",
       "      <td>0.0907</td>\n",
       "      <td>11.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>YOLO11n</td>\n",
       "      <td>NCNN</td>\n",
       "      <td>YOLO11n</td>\n",
       "      <td>0.9294</td>\n",
       "      <td>0.8347</td>\n",
       "      <td>0.8795</td>\n",
       "      <td>0.8821</td>\n",
       "      <td>0.9110</td>\n",
       "      <td>0.6649</td>\n",
       "      <td>166.95</td>\n",
       "      <td>0.0863</td>\n",
       "      <td>11.59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model Name   Backend Eval Type  Precision  Recall  F1-score  Accuracy  \\\n",
       "0     YOLOv5n   PyTorch   YOLOv5n     0.9322  0.8268    0.8763    0.8795   \n",
       "1     YOLOv8n   PyTorch   YOLOv8n     0.9259  0.8300    0.8753    0.8780   \n",
       "2     YOLO11n   PyTorch   YOLO11n     0.9325  0.8345    0.8808    0.8835   \n",
       "3     YOLOv5n      ONNX   YOLOv5n     0.9322  0.8268    0.8763    0.8795   \n",
       "4     YOLOv8n      ONNX   YOLOv8n     0.9139  0.8394    0.8751    0.8767   \n",
       "5     YOLO11n      ONNX   YOLO11n     0.9325  0.8345    0.8808    0.8835   \n",
       "6     YOLOv5n  OpenVINO   YOLOv5n     0.9258  0.8246    0.8723    0.8752   \n",
       "7     YOLOv8n  OpenVINO   YOLOv8n     0.9147  0.8412    0.8764    0.8780   \n",
       "8     YOLO11n  OpenVINO   YOLO11n     0.9289  0.8355    0.8797    0.8822   \n",
       "9     YOLOv5n      NCNN   YOLOv5n     0.9230  0.8268    0.8723    0.8749   \n",
       "10    YOLOv8n      NCNN   YOLOv8n     0.9141  0.8408    0.8759    0.8775   \n",
       "11    YOLO11n      NCNN   YOLO11n     0.9294  0.8347    0.8795    0.8821   \n",
       "\n",
       "    mAP@50  mAP@50-95  Eval Time (s)  Infer Time (s/img)    FPS  \n",
       "0   0.9090     0.6600         552.35              0.3965   2.52  \n",
       "1   0.9127     0.6652         587.73              0.4177   2.39  \n",
       "2   0.9134     0.6680         571.83              0.4034   2.48  \n",
       "3   0.9090     0.6600         309.51              0.3744   2.67  \n",
       "4   0.9109     0.6636         260.83              0.4157   2.41  \n",
       "5   0.9134     0.6680         343.78              0.4665   2.14  \n",
       "6   0.9053     0.6555         158.73              0.1662   6.02  \n",
       "7   0.9108     0.6575         139.67              0.1530   6.54  \n",
       "8   0.9109     0.6649         143.87              0.1797   5.57  \n",
       "9   0.9063     0.6550         159.67              0.0774  12.93  \n",
       "10  0.9109     0.6579         164.79              0.0907  11.03  \n",
       "11  0.9110     0.6649         166.95              0.0863  11.59  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
